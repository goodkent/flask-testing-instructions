<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 11: Performance and Load Testing - Flask Testing Mastery</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/prism.css">
</head>
<body>
    <div class="container">
        <div class="nav-links">
            <a href="chapter10.html">← Previous Chapter</a>
            <a href="index.html">Index</a>
            <a href="chapter12.html">Next Chapter →</a>
        </div>

        <h1>Chapter 11: Performance and Load Testing</h1>
        <p class="chapter-subtitle">Ensuring Your Application Scales Under Real-World Traffic</p>

        <div class="github-links">
            <span class="github-label">Chapter Code:</span>
            <a href="https://github.com/goodkent/flask-testing/tree/ch11-end" class="github-link">Browse</a>
            <a href="https://github.com/goodkent/flask-testing/archive/refs/tags/ch11-end.zip" class="github-link">Zip</a>
            <a href="https://github.com/goodkent/flask-testing/compare/ch10-end...ch11-end" class="github-link">Diff</a>
        </div>

        <div class="intro-section">
            <p>Welcome back! In the previous chapter, you learned how to test your application from a user's perspective using Selenium and E2E tests. Those tests verify that your application <em>works correctly</em>, but there's another crucial question: does it work <em>fast</em>? More importantly, does it work fast when you have 100 concurrent users? 1,000? 10,000?</p>

            <p>If you've ever experienced a website that slows to a crawl during peak hours, you've witnessed the difference between functional code and performant code. Your FlaskBlog Pro application might work perfectly during development with just you testing it, but what happens when it goes viral and gets featured on Hacker News? Will it handle the traffic gracefully, or will it collapse under the load?</p>

            <p>Performance testing answers these questions <em>before</em> you launch. In this chapter, you're going to learn how to simulate realistic traffic using Locust, identify bottlenecks in your application, profile database queries, and optimize your code to handle production-level loads. This isn't about premature optimization—it's about understanding your application's limits and making informed decisions about infrastructure and code improvements.</p>
        </div>

        <h2>Learning Objectives</h2>

        <ul>
            <li>Understand the difference between performance testing and load testing</li>
            <li>Install and configure Locust for testing Flask applications</li>
            <li>Write realistic load test scenarios that simulate actual user behavior</li>
            <li>Identify performance bottlenecks using profiling tools</li>
            <li>Optimize database queries and implement proper indexing</li>
            <li>Validate caching strategies under load</li>
            <li>Interpret load test results and make data-driven optimization decisions</li>
        </ul>

        <h2>Understanding Performance vs Load Testing</h2>

        <p>Before we dive into Locust, let's clarify what we mean by performance testing and load testing. These terms are often used interchangeably, but they focus on different aspects:</p>

        <p><strong>Performance Testing</strong> measures how fast your application responds under normal conditions. It answers questions like: "How long does it take to load the homepage?" or "How quickly can the database retrieve a user's posts?" Performance testing helps you establish a baseline—what "normal" looks like for your application.</p>

        <p><strong>Load Testing</strong> measures how your application behaves under stress. It answers: "Can the server handle 1,000 concurrent users?" or "What happens when 500 people try to create posts simultaneously?" Load testing reveals your application's breaking point and helps you plan for scaling.</p>

        <p>Here's why both matter: You might have a homepage that loads in 50ms under normal conditions (great performance!), but if that shoots up to 5 seconds when you have 100 concurrent users, you have a scaling problem. Conversely, your application might handle 1,000 users without crashing (good load handling), but if every page takes 3 seconds to load even with one user, that's a performance problem.</p>

        <p>In this chapter, we'll focus primarily on load testing using Locust, but we'll also learn how to use profiling tools to identify and fix performance issues that load testing reveals.</p>

        <h2>Setting Up Locust</h2>

        <p>Locust is a powerful, user-friendly load testing tool written in Python. Unlike many load testing tools that use complicated configuration files, Locust lets you define user behavior in regular Python code. This makes it perfect for testing Flask applications—you can use the same Python skills you already have!</p>

        <p>First, let's install Locust in your virtual environment:</p>

        <pre><code class="language-bash">(venv) $ pip install locust</code></pre>

        <p>Locust also provides a web interface for running tests and viewing real-time results. You can start the interface with a simple command, and it gives you beautiful graphs showing request rates, response times, and failure rates as your test runs. But before we can use that interface, we need to tell Locust what to test!</p>

        <h3>Understanding Locust Concepts</h3>

        <p>Locust tests are built around a few key concepts:</p>

        <p><strong>Users:</strong> Simulated users that interact with your application. Each user runs independently and can perform different tasks. Think of these as virtual browsers visiting your site.</p>

        <p><strong>Tasks:</strong> Actions that users perform. These might be loading the homepage, logging in, creating a post, or viewing a profile. You define tasks as Python methods.</p>

        <p><strong>Wait Time:</strong> How long a user pauses between tasks. Real users don't hammer your server non-stop—they read content, think, type, etc. Locust simulates this realistic behavior with wait times.</p>

        <p><strong>Load:</strong> The number of users and how quickly they're spawned. You might start with 10 users and gradually ramp up to 1,000 to see where your application starts to struggle.</p>

        <p>The beauty of Locust is that you define all of this in straightforward Python code. Let's see what a basic Locust test looks like.</p>

        <h3>Creating Your First Locustfile</h3>

        <p>Locust tests live in a file called <code>locustfile.py</code>. Let's create a simple one that tests our FlaskBlog Pro homepage:</p>

        <pre><code class="language-python"># locustfile.py: Basic load test for homepage
from locust import HttpUser, task, between

class BlogUser(HttpUser):
    """Simulates a user visiting FlaskBlog Pro."""
    
    # Wait 1-3 seconds between requests (realistic user behavior)
    wait_time = between(1, 3)
    
    @task
    def load_homepage(self):
        """User visits the homepage."""
        self.client.get("/")</code></pre>

        <p>Let's break down what's happening here:</p>

        <p>We create a class that inherits from <code>HttpUser</code>. This represents a user visiting our site. The <code>wait_time = between(1, 3)</code> means each simulated user will wait a random time between 1 and 3 seconds before performing their next task—just like real users don't immediately click after a page loads.</p>

        <p>The <code>@task</code> decorator marks a method as a task that users will perform. In this case, our users simply visit the homepage with <code>self.client.get("/")</code>. The <code>self.client</code> is similar to the test client you've been using throughout this course—it makes HTTP requests to your application.</p>

        <p>Notice we don't specify the host URL in our code. That's because you provide it when you run Locust, making it easy to test against different environments (development, staging, production).</p>

        <h3>Running Your First Load Test</h3>

        <p>Before running a load test, make sure your Flask application is running. Open a terminal and start it:</p>

        <pre><code class="language-bash">(venv) $ flask run</code></pre>

        <p>Now, in a separate terminal (with your virtual environment activated), run Locust:</p>

        <pre><code class="language-bash">(venv) $ locust -f locustfile.py --host=http://localhost:5000</code></pre>

        <p>You should see output like this:</p>

        <pre><code class="language-bash">[2024-12-18 10:30:15,123] INFO/locust.main: Starting web interface at http://0.0.0.0:8089
[2024-12-18 10:30:15,125] INFO/locust.main: Starting Locust 2.20.0</code></pre>

        <p>Locust is now running! Open your browser and visit <code>http://localhost:8089</code>. You'll see a clean interface asking you how many users to simulate and how quickly to spawn them. Let's start conservatively:</p>

        <ul>
            <li><strong>Number of users (peak concurrency):</strong> 50</li>
            <li><strong>Spawn rate:</strong> 5 users/second</li>
        </ul>

        <p>Click "Start swarming" and watch what happens. You'll see real-time graphs showing:</p>

        <ul>
            <li><strong>Total Requests per Second (RPS):</strong> How many requests your server is handling</li>
            <li><strong>Response Times:</strong> How long requests take (median, 95th percentile, 99th percentile)</li>
            <li><strong>Number of Users:</strong> How many simulated users are currently active</li>
        </ul>

        <p>For a simple homepage request with 50 concurrent users, you should see response times under 100ms on a typical development machine. If you're seeing response times over 1 second for this simple test, something is wrong—you might have a performance issue even before considering load!</p>

        <div class="note">
            <strong>Development vs Production Servers:</strong> If you're running Flask's development server (<code>flask run</code>), you're using Werkzeug's single-threaded server. This is fine for development and learning, but it can't handle high concurrency. In production, you'd use a real WSGI server like Gunicorn or uWSGI. For more realistic load testing, you can install Gunicorn (<code>pip install gunicorn</code>) and run <code>gunicorn -w 4 -b 127.0.0.1:5000 "app:create_app()"</code> to get multi-worker concurrency that better simulates production.
        </div>

        <h2>Realistic Load Testing Scenarios</h2>

        <p>Testing just the homepage is a good start, but it doesn't represent realistic user behavior. Real users navigate around your site, create content, log in, etc. Let's create a more realistic test scenario that simulates typical FlaskBlog Pro usage.</p>

        <h3>Simulating Mixed User Behaviors</h3>

        <p>In reality, not all users do the same thing. Some browse, some read posts, some create content. Locust lets you model this with task weights—tasks with higher weights run more frequently.</p>

        <pre><code class="language-python"># locustfile.py: Realistic FlaskBlog Pro load test
from locust import HttpUser, task, between
import random

class BlogReader(HttpUser):
    """Simulates a user who primarily reads content."""
    
    wait_time = between(1, 5)
    
    @task(3)
    def view_posts_list(self):
        """Browse the posts list (happens often)."""
        self.client.get("/posts")
    
    @task(2)
    def view_post_detail(self):
        """Read a specific post (happens frequently)."""
        post_id = random.randint(1, 100)
        self.client.get(f"/posts/{post_id}")
    
    @task(1)
    def view_user_profile(self):
        """Check out a user's profile (less common)."""
        user_id = random.randint(1, 50)
        self.client.get(f"/users/{user_id}")
    
    def on_start(self):
        """Called when a user starts. Simulates visiting homepage first."""
        self.client.get("/")</code></pre>

        <p>Let's understand the weights: <code>@task(3)</code> means this task is three times more likely to run than a <code>@task(1)</code> task. So in our scenario, users browse the posts list most often (weight 3), read individual posts pretty often (weight 2), and occasionally check profiles (weight 1). This 3:2:1 ratio roughly simulates real blog behavior.</p>

        <p>The <code>on_start()</code> method runs once when a user begins their session—we use it to simulate that most users start by visiting the homepage. After that, they perform the weighted tasks randomly.</p>

        <h3>Testing Authenticated User Behavior</h3>

        <p>Many of FlaskBlog Pro's features require authentication. Let's create a user class that logs in and performs authenticated actions:</p>

        <pre><code class="language-python"># Add to locustfile.py: Authenticated user behavior
class BlogContributor(HttpUser):
    """Simulates a logged-in user who creates content."""
    
    wait_time = between(2, 8)
    
    def on_start(self):
        """Login before performing authenticated tasks."""
        self.client.post("/auth/login", data={
            "email": "test@example.com",
            "password": "testpass123"
        })
    
    @task(2)
    def view_dashboard(self):
        """Check personal dashboard."""
        self.client.get("/dashboard")
    
    @task(1)
    def create_post(self):
        """Write a new blog post."""
        self.client.post("/posts/create", data={
            "title": f"Load Test Post {random.randint(1, 10000)}",
            "content": "This is a test post created during load testing.",
            "category": random.choice(["tech", "lifestyle", "travel"])
        })
    
    @task(1)
    def edit_own_post(self):
        """Edit an existing post."""
        post_id = random.randint(1, 50)
        self.client.post(f"/posts/{post_id}/edit", data={
            "title": "Updated Title",
            "content": "Updated content."
        })</code></pre>

        <p>This user logs in during <code>on_start()</code> and then performs authenticated actions. Notice the longer wait times (2-8 seconds) compared to readers—content creators spend more time between actions as they write posts, think about edits, etc.</p>

        <div class="note">
            <strong>Using Real Test Accounts:</strong> In this example, we're using a hardcoded test account. In a real test, you might want to create test accounts dynamically or use a pool of pre-created test users. For now, make sure you have a test account in your database with these credentials before running this load test. You can create one with: <code>flask shell</code> and then use your User model to create a test user.
        </div>

        <h2>Identifying Performance Bottlenecks</h2>

        <p>Running a load test is enlightening, but it only tells you <em>that</em> your application is slow, not <em>why</em>. To find the bottlenecks, you need profiling. Flask-DebugToolbar is excellent during development, but for load testing, we need something more powerful: the Werkzeug profiler and Flask-Profiler.</p>

        <h3>Using Flask's Built-In Profiler</h3>

        <p>Flask has a built-in profiler that can show you exactly where your application spends time. Let's add it to our configuration:</p>

        <pre><code class="language-python"># config.py: Add profiling configuration
class TestingConfig(Config):
    TESTING = True
    SQLALCHEMY_DATABASE_URI = 'sqlite:///:memory:'
    WTF_CSRF_ENABLED = False
    PROFILE = True  # Enable profiling

class DevelopmentConfig(Config):
    DEBUG = True
    SQLALCHEMY_DATABASE_URI = 'sqlite:///dev.db'
    PROFILE = True  # Enable profiling for development too</code></pre>

        <p>Now we can use Werkzeug's ProfilerMiddleware to profile our application. Create a new file for profiling:</p>

        <pre><code class="language-python"># profile_app.py: Run Flask with profiler
from werkzeug.middleware.profiler import ProfilerMiddleware
from app import create_app

app = create_app('development')
app.wsgi_app = ProfilerMiddleware(app.wsgi_app, restrictions=[30])

if __name__ == '__main__':
    app.run(debug=True)</code></pre>

        <p>Run this with <code>python profile_app.py</code>, then visit a few routes. You'll see profiling output in your terminal showing the slowest functions. The <code>restrictions=[30]</code> parameter limits output to the top 30 slowest functions.</p>

        <h3>Using Flask-Profiler for Visual Analysis</h3>

        <p>For a more user-friendly experience, install Flask-Profiler:</p>

        <pre><code class="language-bash">(venv) $ pip install flask_profiler</code></pre>

        <p>Add it to your application factory:</p>

        <pre><code class="language-python"># app/__init__.py: Add Flask-Profiler
import flask_profiler

def create_app(config_name='development'):
    app = Flask(__name__)
    app.config.from_object(config[config_name])
    
    # Initialize extensions...
    
    # Configure profiler (only in development)
    if config_name == 'development':
        app.config['flask_profiler'] = {
            'enabled': True,
            'storage': {
                'engine': 'sqlite',
                'FILE': 'flask_profiler.db'
            },
            'basicAuth': {
                'enabled': False  # Set to True in production
            }
        }
        flask_profiler.init_app(app)
    
    return app</code></pre>

        <p>Now when you run your application and visit routes, Flask-Profiler tracks them. Visit <code>http://localhost:5000/flask-profiler/</code> to see a beautiful dashboard showing:</p>

        <ul>
            <li>Which routes are slowest</li>
            <li>How many times each route was called</li>
            <li>Average, min, max response times</li>
            <li>Detailed breakdown of time spent in each function</li>
        </ul>

        <p>This is incredibly valuable! You can run a load test with Locust, then check Flask-Profiler to see exactly which routes are slow and why.</p>

        <h2>Optimizing Database Queries</h2>

        <p>In most web applications, the database is the primary performance bottleneck. A slow query can make your entire application crawl. Let's learn how to identify and fix slow database queries.</p>

        <h3>Identifying N+1 Query Problems</h3>

        <p>One of the most common database performance issues is the N+1 query problem. Here's what it looks like:</p>

        <pre><code class="language-python"># app/routes.py: N+1 query problem example (BAD!)
@bp.route('/posts')
def posts_list():
    posts = Post.query.all()  # 1 query to get all posts
    return render_template('posts.html', posts=posts)

# In the template:
# {% for post in posts %}
#   {{ post.author.username }}  # 1 query PER POST to get author!
# {% endfor %}</code></pre>

        <p>If you have 100 posts, this code executes 101 database queries: 1 to get all posts, then 100 more to get each post's author. That's terrible for performance!</p>

        <p>The solution is eager loading with <code>joinedload</code> or <code>subqueryload</code>:</p>

        <pre><code class="language-python"># app/routes.py: Fixed with eager loading (GOOD!)
from sqlalchemy.orm import joinedload

@bp.route('/posts')
def posts_list():
    posts = Post.query.options(joinedload(Post.author)).all()
    return render_template('posts.html', posts=posts)</code></pre>

        <p>Now SQLAlchemy fetches both posts and their authors in just 1 or 2 queries using SQL JOINs. Much faster!</p>

        <h3>Adding Database Indexes</h3>

        <p>Database indexes are like a book's index—they let the database find data quickly without scanning every row. If you're frequently querying by a column, it should probably have an index.</p>

        <p>Let's say your FlaskBlog Pro has a route that shows posts by category:</p>

        <pre><code class="language-python"># app/routes.py: Querying by category
@bp.route('/posts/category/&lt;category&gt;')
def posts_by_category(category):
    posts = Post.query.filter_by(category=category).all()
    return render_template('posts.html', posts=posts)</code></pre>

        <p>If the <code>category</code> column doesn't have an index, the database has to check every single post to find matches. With 100,000 posts, that's slow. Let's add an index:</p>

        <pre><code class="language-python"># app/models.py: Add index to Post model
class Post(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    title = db.Column(db.String(200), nullable=False)
    content = db.Column(db.Text, nullable=False)
    category = db.Column(db.String(50), index=True)  # Add index=True
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    user_id = db.Column(db.Integer, db.ForeignKey('user.id'))</code></pre>

        <p>Now queries filtering by category are much faster! The database can use the index to jump directly to matching posts instead of scanning everything.</p>

        <h3>Monitoring Query Performance</h3>

        <p>SQLAlchemy can show you all the SQL queries your application executes. Add this to your config during load testing:</p>

        <pre><code class="language-python"># config.py: Enable SQL query logging
class DevelopmentConfig(Config):
    DEBUG = True
    SQLALCHEMY_DATABASE_URI = 'sqlite:///dev.db'
    SQLALCHEMY_ECHO = True  # Show all SQL queries in terminal</code></pre>

        <p>When <code>SQLALCHEMY_ECHO = True</code>, you'll see every SQL query in your terminal. Run a load test and watch the logs—if you see the same query repeated hundreds of times, that's your N+1 problem!</p>

        <h2>Testing Caching Strategies</h2>

        <p>Caching is one of the most effective ways to improve performance. Instead of regenerating the same data over and over, you cache it and serve it quickly. But how do you know your caching is working correctly? Load testing!</p>

        <h3>Implementing Flask-Caching</h3>

        <p>First, let's add caching to FlaskBlog Pro:</p>

        <pre><code class="language-bash">(venv) $ pip install Flask-Caching</code></pre>

        <pre><code class="language-python"># app/__init__.py: Initialize caching
from flask_caching import Cache

cache = Cache()

def create_app(config_name='development'):
    app = Flask(__name__)
    app.config.from_object(config[config_name])
    
    # Configure cache
    app.config['CACHE_TYPE'] = 'SimpleCache'  # Use Redis in production
    app.config['CACHE_DEFAULT_TIMEOUT'] = 300  # 5 minutes
    
    cache.init_app(app)
    
    # Initialize other extensions...
    
    return app</code></pre>

        <p>Now let's cache our posts list route:</p>

        <pre><code class="language-python"># app/routes.py: Cache the posts list
from app import cache

@bp.route('/posts')
@cache.cached(timeout=60, key_prefix='all_posts')
def posts_list():
    posts = Post.query.options(joinedload(Post.author)).all()
    return render_template('posts.html', posts=posts)</code></pre>

        <p>The first request to <code>/posts</code> will execute the database query and cache the result. For the next 60 seconds, subsequent requests will be served from cache—no database query at all! This is a huge performance win.</p>

        <h3>Load Testing Cached Routes</h3>

        <p>Here's where load testing proves caching works. Let's create a load test that specifically targets our cached route:</p>

        <pre><code class="language-python"># locustfile.py: Test caching effectiveness
class CacheTestUser(HttpUser):
    """Tests that caching improves performance."""
    
    wait_time = between(0.1, 0.5)  # Rapid requests to test cache
    
    @task
    def test_cached_posts(self):
        """Hammer the cached posts route."""
        with self.client.get("/posts", catch_response=True) as response:
            if response.elapsed.total_seconds() > 0.1:
                response.failure(f"Request took {response.elapsed.total_seconds()}s, cache might not be working!")
            else:
                response.success()</code></pre>

        <p>This test makes rapid requests (0.1-0.5 second wait time) to the posts route and fails if a request takes longer than 100ms. If caching is working, most requests should be nearly instant (served from memory). If caching isn't working, you'll see failures because database queries take longer.</p>

        <p>Run this test with 100 users and watch your response times. With caching disabled, you might see 200-500ms response times. With caching enabled, you should see 5-20ms response times—a 10-40x improvement!</p>

        <h2>Interpreting Load Test Results</h2>

        <p>Now that you know how to run load tests, let's talk about interpreting the results. Locust gives you lots of data, but what does it mean? What's good performance vs bad performance?</p>

        <h3>Key Metrics to Watch</h3>

        <p><strong>Response Time Percentiles:</strong> Locust shows 50th, 95th, and 99th percentile response times. The median (50th) tells you what a typical user experiences. The 95th and 99th percentiles tell you what your slowest users experience. A good application has low 95th percentile times—even your unluckiest users should have a decent experience.</p>

        <p>For example:</p>
        <ul>
            <li><strong>Good:</strong> Median 50ms, 95th percentile 150ms, 99th percentile 300ms</li>
            <li><strong>Warning:</strong> Median 200ms, 95th percentile 1s, 99th percentile 3s</li>
            <li><strong>Bad:</strong> Median 500ms, 95th percentile 3s, 99th percentile 10s</li>
        </ul>

        <p><strong>Requests Per Second (RPS):</strong> This tells you how much traffic your application can handle. A simple Flask app on a basic server might handle 100-500 RPS. With optimization and proper infrastructure (Gunicorn with multiple workers, caching, CDN), you might reach 1,000-5,000 RPS. For high-traffic sites, you'd need load balancing and horizontal scaling to reach 10,000+ RPS.</p>

        <p><strong>Failure Rate:</strong> This is critical. Even one failure is concerning during a load test. Failures indicate your application is breaking under load—timeouts, 500 errors, database connection pool exhaustion, etc. A healthy application should have a 0% failure rate even under heavy load.</p>

        <h3>Establishing Your Performance Baseline</h3>

        <p>Before you optimize anything, establish a baseline. Run a load test with your current code and record the results:</p>

        <pre><code class="language-bash"># Example baseline results
Users: 100
RPS: 45
Median response time: 380ms
95th percentile: 1.2s
99th percentile: 2.8s
Failure rate: 0%</code></pre>

        <p>This gives you a reference point. Now when you make optimizations (add caching, fix N+1 queries, add indexes), run the test again. You should see improvements:</p>

        <pre><code class="language-bash"># After optimization
Users: 100
RPS: 220 (4.8x improvement!)
Median response time: 45ms (8.4x faster!)
95th percentile: 120ms (10x faster!)
99th percentile: 250ms (11x faster!)
Failure rate: 0%</code></pre>

        <p>Numbers like this prove your optimizations work! You're not guessing—you have data showing that your changes made the application significantly faster.</p>

        <h3>Finding Your Application's Breaking Point</h3>

        <p>A valuable exercise is finding where your application breaks. Gradually increase the number of users until you see:</p>

        <ul>
            <li>Response times spike dramatically</li>
            <li>Failure rate increases above 0%</li>
            <li>RPS stops increasing even as you add more users</li>
        </ul>

        <p>This is your breaking point. For example, you might find that your application handles 500 concurrent users fine, but at 600 users, response times shoot up and failures begin. This tells you that you need to scale (add more servers, upgrade database, implement load balancing) before reaching that traffic level.</p>

        <div class="troubleshooting">
            <h2>Troubleshooting Load Tests</h2>

            <h3>Problem: Load test shows 100% failures</h3>
            <p><strong>Solution:</strong> Check that your Flask application is actually running and accessible at the host you specified. Also verify the routes in your locustfile match your actual routes. Try manually visiting the URLs in your browser first.</p>

            <h3>Problem: Response times are consistently very high (multiple seconds) even with few users</h3>
            <p><strong>Solution:</strong> This indicates a performance problem independent of load. Use Flask-Profiler to identify slow functions. Common culprits: missing database indexes, N+1 queries, slow external API calls. Fix these first before load testing.</p>

            <h3>Problem: Application crashes after a few minutes of load testing</h3>
            <p><strong>Solution:</strong> Check for resource leaks. Common issues:</p>
            <ul>
                <li><strong>Database connections not being closed:</strong> Use SQLAlchemy's scoped sessions correctly</li>
                <li><strong>Memory leaks:</strong> Large objects not being garbage collected</li>
                <li><strong>File descriptors:</strong> Opening files without closing them</li>
            </ul>
            <p>Run your application with memory profiling tools to identify leaks.</p>

            <h3>Problem: Locust shows good RPS but my application seems slow</h3>
            <p><strong>Solution:</strong> You might be hitting a different bottleneck. Check:</p>
            <ul>
                <li>Is your database on the same machine? Database I/O might be slow</li>
                <li>Are you running Flask's development server? Use Gunicorn instead</li>
                <li>Is your machine's CPU pegged at 100%? You've hit your hardware limit</li>
            </ul>

            <h3>Problem: Tests work fine locally but fail in production</h3>
            <p><strong>Solution:</strong> Production environments have different characteristics:</p>
            <ul>
                <li>Network latency between app server and database</li>
                <li>Different server specifications (CPU, RAM)</li>
                <li>Security software or firewalls affecting performance</li>
            </ul>
            <p>Ideally, run load tests in a staging environment that mirrors production.</p>
        </div>

        <h2>Hands-On Exercise: Optimize FlaskBlog Pro</h2>

        <div class="exercise">
            <h3>Your Mission</h3>

            <p>In this exercise, you're going to find and fix performance bottlenecks in FlaskBlog Pro. You'll use Locust to establish a baseline, identify problems with profiling, fix them, and measure the improvement.</p>

            <h3>Setup</h3>

            <p>First, create a script to populate your database with test data. You need enough data to see performance issues:</p>

            <pre><code class="language-python"># populate_db.py: Create test data
from app import create_app, db
from app.models import User, Post, Comment
from datetime import datetime, timedelta
import random

app = create_app('development')

with app.app_context():
    # Clear existing data
    db.drop_all()
    db.create_all()
    
    # Create 50 users
    users = []
    for i in range(50):
        user = User(
            username=f"user{i}",
            email=f"user{i}@example.com"
        )
        user.set_password("password123")
        users.append(user)
        db.session.add(user)
    
    db.session.commit()
    print(f"Created {len(users)} users")
    
    # Create 500 posts
    categories = ['tech', 'lifestyle', 'travel', 'food', 'fitness']
    posts = []
    for i in range(500):
        post = Post(
            title=f"Blog Post {i}",
            content=f"This is the content for blog post number {i}. " * 20,
            category=random.choice(categories),
            author=random.choice(users),
            created_at=datetime.utcnow() - timedelta(days=random.randint(0, 365))
        )
        posts.append(post)
        db.session.add(post)
    
    db.session.commit()
    print(f"Created {len(posts)} posts")
    
    # Create 2000 comments
    for i in range(2000):
        comment = Comment(
            content=f"This is comment number {i}",
            post=random.choice(posts),
            author=random.choice(users),
            created_at=datetime.utcnow() - timedelta(days=random.randint(0, 30))
        )
        db.session.add(comment)
    
    db.session.commit()
    print("Created 2000 comments")
    print("Database population complete!")</code></pre>

            <p>Run this script: <code>python populate_db.py</code></p>

            <h3>Step 1: Establish Baseline Performance</h3>

            <p>Create a comprehensive load test file:</p>

            <pre><code class="language-python"># locustfile.py: FlaskBlog Pro load test
from locust import HttpUser, task, between
import random

class FlaskBlogUser(HttpUser):
    wait_time = between(1, 3)
    
    @task(5)
    def view_posts_list(self):
        """Most common action: browsing posts."""
        self.client.get("/posts")
    
    @task(3)
    def view_post_detail(self):
        """Reading a specific post."""
        post_id = random.randint(1, 500)
        with self.client.get(f"/posts/{post_id}", catch_response=True) as response:
            if response.status_code == 404:
                response.success()  # Post might not exist, that's okay
    
    @task(2)
    def view_posts_by_category(self):
        """Browse posts by category."""
        category = random.choice(['tech', 'lifestyle', 'travel', 'food', 'fitness'])
        self.client.get(f"/posts/category/{category}")
    
    @task(1)
    def view_user_profile(self):
        """Check a user's profile."""
        user_id = random.randint(1, 50)
        self.client.get(f"/users/{user_id}")
    
    def on_start(self):
        """Start by visiting the homepage."""
        self.client.get("/")</code></pre>

            <p>Run the test with 50 users and record your baseline:</p>

            <pre><code class="language-bash">(venv) $ locust -f locustfile.py --host=http://localhost:5000</code></pre>

            <p>Visit <code>http://localhost:8089</code>, start with 50 users (spawn rate: 5), and let it run for 2-3 minutes. Note your results:</p>

            <ul>
                <li>Median response time</li>
                <li>95th percentile response time</li>
                <li>Requests per second</li>
                <li>Any failures</li>
            </ul>

            <h3>Step 2: Identify Bottlenecks with Profiling</h3>

            <p>Enable SQL query logging and Flask-Profiler as shown earlier in this chapter. Run your application and navigate to a few routes manually:</p>

            <ol>
                <li>Visit <code>/posts</code> and watch your terminal for SQL queries</li>
                <li>Count how many queries execute for the posts list</li>
                <li>Visit <code>/posts/1</code> and count queries for post detail</li>
                <li>Visit <code>/posts/category/tech</code> and check if queries are slow</li>
            </ol>

            <p>Look for these red flags:</p>
            <ul>
                <li>N+1 queries: The same SELECT query repeated many times</li>
                <li>Slow queries: Any query taking >100ms (without indexes)</li>
                <li>Missing JOINs: Loading related data in separate queries</li>
            </ul>

            <h3>Step 3: Fix the Bottlenecks</h3>

            <p>Based on what you found, implement these fixes:</p>

            <p><strong>Fix N+1 Queries:</strong></p>
            <pre><code class="language-python"># app/routes.py: Fix posts list N+1 query
from sqlalchemy.orm import joinedload

@bp.route('/posts')
def posts_list():
    # Load posts with their authors in one query
    posts = Post.query.options(
        joinedload(Post.author)
    ).order_by(Post.created_at.desc()).all()
    return render_template('posts/list.html', posts=posts)</code></pre>

            <p><strong>Add Database Indexes:</strong></p>
            <pre><code class="language-python"># app/models.py: Add indexes to frequently queried columns
class Post(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    title = db.Column(db.String(200), nullable=False)
    content = db.Column(db.Text, nullable=False)
    category = db.Column(db.String(50), index=True)  # Index for category filtering
    created_at = db.Column(db.DateTime, default=datetime.utcnow, index=True)  # Index for sorting
    user_id = db.Column(db.Integer, db.ForeignKey('user.id'))  # Already indexed (foreign key)</code></pre>

            <p><strong>Add Caching:</strong></p>
            <pre><code class="language-python"># app/routes.py: Cache category pages
@bp.route('/posts/category/&lt;category&gt;')
@cache.cached(timeout=120, key_prefix=lambda: f'category_{category}')
def posts_by_category(category):
    posts = Post.query.filter_by(category=category)\
        .options(joinedload(Post.author))\
        .order_by(Post.created_at.desc())\
        .all()
    return render_template('posts/list.html', posts=posts, category=category)</code></pre>

            <h3>Step 4: Measure the Improvement</h3>

            <p>After implementing your fixes:</p>

            <ol>
                <li>Recreate your database with indexes: <code>flask db migrate</code> and <code>flask db upgrade</code></li>
                <li>Repopulate test data: <code>python populate_db.py</code></li>
                <li>Run the same Locust test with the same parameters</li>
                <li>Compare results to your baseline</li>
            </ol>

            <p>You should see significant improvements:</p>
            <ul>
                <li><strong>Response times:</strong> 3-10x faster due to fewer queries and better indexes</li>
                <li><strong>RPS:</strong> 2-5x higher due to faster processing</li>
                <li><strong>Consistency:</strong> Lower 95th/99th percentiles showing more consistent performance</li>
            </ul>

            <h3>Success Criteria</h3>

            <p>Your exercise is complete when:</p>
            <ul>
                <li>You've established a baseline with at least 3 key metrics</li>
                <li>You've identified at least 2 bottlenecks (N+1 queries, missing indexes, etc.)</li>
                <li>You've implemented fixes for those bottlenecks</li>
                <li>You've measured improvements showing at least 2x faster response times</li>
                <li>You can explain why each optimization helped</li>
                <li>Your load test shows 0% failure rate under normal load</li>
            </ul>
        </div>

        <h2>What We've Learned</h2>

        <p>Take a moment to appreciate everything you've accomplished in this chapter:</p>

        <ul>
            <li>✓ You understand the difference between performance testing and load testing, and when each is appropriate</li>
            <li>✓ You can install and configure Locust to test Flask applications with realistic user behavior</li>
            <li>✓ You know how to write Locust test scenarios with weighted tasks that simulate mixed user behaviors</li>
            <li>✓ You've learned to test authenticated user flows by logging in during the on_start method</li>
            <li>✓ You can use Flask-Profiler and Werkzeug's profiler to identify performance bottlenecks</li>
            <li>✓ You understand the N+1 query problem and how to fix it with eager loading (joinedload)</li>
            <li>✓ You know when and how to add database indexes to improve query performance</li>
            <li>✓ You can implement Flask-Caching and verify that caching strategies work under load</li>
            <li>✓ You've learned to interpret load test results: response time percentiles, RPS, and failure rates</li>
            <li>✓ You can establish performance baselines and measure improvements after optimization</li>
            <li>✓ You know how to find your application's breaking point and plan for scaling</li>
            <li>✓ You can use SQLALCHEMY_ECHO to monitor database queries during development and testing</li>
        </ul>

        <p>Performance testing is often overlooked until it's too late—until the application is live and users are complaining about slow load times. By learning these techniques now, you're ahead of the curve! You can identify and fix performance issues during development, long before they affect real users. The ability to measure, optimize, and prove improvements with data is what separates good developers from great ones. FlaskBlog Pro is not just functional—it's fast, scalable, and ready for production traffic!</p>

        <h2>Before You Continue...</h2>

        <p>Before moving on to Chapter 12, make sure you:</p>

        <ol>
            <li>Can explain the difference between performance testing (measuring speed) and load testing (measuring behavior under stress)</li>
            <li>Have Locust installed and can write basic load test scenarios with weighted tasks</li>
            <li>Understand how to simulate realistic user behavior including wait times and on_start methods</li>
            <li>Know how to use Flask-Profiler or Werkzeug's profiler to identify slow functions and routes</li>
            <li>Can identify N+1 query problems and fix them with joinedload or subqueryload</li>
            <li>Understand when to add database indexes and how they improve query performance</li>
            <li>Can implement Flask-Caching and verify caching effectiveness through load testing</li>
            <li>Know how to interpret response time percentiles (50th, 95th, 99th) and what they tell you about user experience</li>
            <li>Can establish a performance baseline before optimization and measure improvements afterward</li>
            <li>Have completed the hands-on exercise: finding bottlenecks and measuring optimization results</li>
        </ol>

        <p>If something doesn't make complete sense yet, that's okay! Performance optimization is an advanced topic that takes practice to master. The key insights are: (1) measure before optimizing—establish baselines so you know if changes help, (2) database queries are usually the bottleneck, (3) N+1 queries and missing indexes are the most common culprits, and (4) caching can provide 10-100x performance improvements for read-heavy routes. Everything else builds on these foundations. You'll develop intuition for performance as you work on more projects!</p>

        <div class="success">
            <strong>Next Up:</strong> In Chapter 12, you're going to learn about test organization and best practices. You've written hundreds of tests throughout this course, but how do you keep them organized and maintainable? You'll discover naming conventions, fixture organization strategies, and how to structure your test suite for long-term success. You'll also learn about test coverage analysis with pytest-cov and how to identify untested code. Good test organization makes the difference between a test suite that helps development and one that becomes a burden!
        </div>

        <div class="nav-links">
            <a href="chapter10.html">← Previous Chapter</a>
            <a href="index.html">Index</a>
            <a href="chapter12.html">Next Chapter →</a>
        </div>

        <footer>
            <p>Flask Testing Mastery - A comprehensive course on testing Flask applications</p>
            <p style="font-size: 0.9em;">Questions or feedback? Let me know!</p>
        </footer>
    </div>

    <!-- Prism.js for syntax highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
</body>
</html>